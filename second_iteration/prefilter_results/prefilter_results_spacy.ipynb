{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "allowed_specials = ['&', ' ', '`']\n",
    "path_to_file = \"../data/labeled_data_1000rows.csv\"\n",
    "path_to_old_file = \"../data/scrpaing_result_raw.csv\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_file, encoding='utf8', sep='|', nrows=628, header=1, names=['text', 'entities'])\n",
    "df.astype('string')\n",
    "df = df.fillna('')\n",
    "df['entities'] = df['entities'].replace('restaurants', '')\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reformat file\n",
    "Split rows with multiple sentences into separate rows (explode data). It might yield better results than long paragraphs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(row['category'], row['text'].split('. ')) for _, row in df.iterrows()])\\\n",
    "    .reset_index().to_csv('../data/labeled_data_1000rows.csv', sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read reformatted scraped dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['nlp'] = df['text'].apply(nlp)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLTK Porter Stemmer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "df['porter_stemms'] = df['nlp'].apply(lambda x: [porter_stemmer.stem(word.text) for word in x])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% NLTK Porter Stemmer\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLTK Snowball Stemmer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "df['snowball_stemms'] = df['nlp'].apply(lambda x: [snowball_stemmer.stem(word.text) for word in x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% NLTK Snowball Stemmer\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SpaCy Lemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['spacy_lemmas'] = df['nlp'].apply(lambda x: [word.lemma_ for word in x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% SpaCy Lemmatizer\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "topic_count = 4\n",
    "max_df = 0.9\n",
    "min_df = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english')\n",
    "dtm = cv.fit_transform(df['text'])\n",
    "dtm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=topic_count, random_state=2137)\n",
    "lda.fit_transform(dtm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get 10 most important words for topics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {index}\")\n",
    "    for i in topic.argsort()[-10:]:\n",
    "        print(cv.get_feature_names()[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get probaility of line belonging to a topic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_result = lda.transform(dtm)\n",
    "topic_result[0].round(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['topic'] = topic_result.argmax(axis=1)\n",
    "df.head(n=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for topic_index in range(topic_count):\n",
    "    topic_row_count = df[df['topic'] == topic_index].count()['text']\n",
    "    entity_row_count = df[(df['entities'] != '') & (df['topic'] == topic_index)].count()['entities']\n",
    "    print(f\"Topic {topic_index}: {entity_row_count}/{topic_row_count}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "None of the found topics cover significant majority of rows containing entity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for topic_count in [2]:\n",
    "    for max_df in [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "        for min_df in [1, 2, 3, 4]:\n",
    "            print(f\"topic_count:{topic_count} max_df:{max_df} min_df:{min_df}\")\n",
    "            cv = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english')\n",
    "            dtm = cv.fit_transform(df['text'])\n",
    "            lda = LatentDirichletAllocation(n_components=topic_count, random_state=2137)\n",
    "            lda.fit_transform(dtm)\n",
    "            topic_result = lda.transform(dtm)\n",
    "            df['topic'] = topic_result.argmax(axis=1)\n",
    "            for topic_index in range(topic_count):\n",
    "                topic_row_count = df[df['topic'] == topic_index].count()['text']\n",
    "                entity_row_count = df[(df['entities'] != '') & (df['topic'] == topic_index)].count()['entities']\n",
    "                print(f\"Topic {topic_index}: {entity_row_count}/{topic_row_count}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use whitelist and blacklsit to select relevant sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "whitelist = [' at ', 'in the', 'crew', 'restaurant', 'residency', 'resident', 'concept', 'launch', 'open', 'chef',\n",
    "             'acclaimed', 'partnership', 'place', 'priced', 'pricey', 'cafe', 'bar ', 'spot', 'location', 'instagram',\n",
    "             'affordable', 'food heaven', 'list', 'highlight', 'visit', 'by the', 'I love', 'At ', 'discover']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use lists on raw text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "for index, row in df.iterrows():\n",
    "    positive = False\n",
    "    for word in whitelist:\n",
    "        if word in row['text'] and row['entities'] != '':\n",
    "            TP += 1\n",
    "            positive = True\n",
    "            break\n",
    "        if word in row['text'] and row['entities'] == '':\n",
    "            FP += 1\n",
    "            positive = True\n",
    "            break\n",
    "    if not positive and row['entities'] == '':\n",
    "        TN += 1\n",
    "    if not positive and row['entities'] != '':\n",
    "        FN += 1\n",
    "        print(row['text'])\n",
    "print(TP, TN, FP, FN)\n",
    "array = [[TP, TN],\n",
    "        [FP, FN]]\n",
    "plot = make_confusion_matrix(np.array(array),\n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      filename='../plots/raw_text_matrix_prefilter.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use lists on porter stemms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "whitelist_porter_stemms = [porter_stemmer.stem(word).strip() for word in whitelist]\n",
    "print(whitelist_porter_stemms)\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "for index, row in df.iterrows():\n",
    "    positive = False\n",
    "    for word in whitelist_porter_stemms:\n",
    "        if word in row['porter_stemms'] and row['entities'] != '':\n",
    "            TP += 1\n",
    "            positive = True\n",
    "            break\n",
    "        if word in row['porter_stemms'] and row['entities'] == '':\n",
    "            FP += 1\n",
    "            positive = True\n",
    "            break\n",
    "    if not positive and row['entities'] == '':\n",
    "        TN += 1\n",
    "    if not positive and row['entities'] != '':\n",
    "        FN += 1\n",
    "        print(row['porter_stemms'])\n",
    "print(TP, TN, FP, FN)\n",
    "array = [[TP, TN],\n",
    "        [FP, FN]]\n",
    "plot = make_confusion_matrix(np.array(array),\n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      filename='../plots/porter_matrix_prefilter.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use lists on snowball stemms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "whitelist_snowball_stemms = [porter_stemmer.stem(word).strip() for word in whitelist]\n",
    "print(whitelist_snowball_stemms)\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "for index, row in df.iterrows():\n",
    "    positive = False\n",
    "    for word in whitelist_snowball_stemms:\n",
    "        if word in row['snowball_stemms'] and row['entities'] != '':\n",
    "            TP += 1\n",
    "            positive = True\n",
    "            break\n",
    "        if word in row['snowball_stemms'] and row['entities'] == '':\n",
    "            FP += 1\n",
    "            positive = True\n",
    "            break\n",
    "    if not positive and row['entities'] == '':\n",
    "        TN += 1\n",
    "    if not positive and row['entities'] != '':\n",
    "        FN += 1\n",
    "        print(row['snowball_stemms'])\n",
    "print(TP, TN, FP, FN)\n",
    "array = [[TP, TN],\n",
    "        [FP, FN]]\n",
    "plot = make_confusion_matrix(np.array(array),\n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      filename='../plots/snowball_matrix_prefilter.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use regexps and other structural characteristics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def is_relevant(word_list):\n",
    "    word_list = word_list[1:]\n",
    "    if any([word[0].isupper() for word in word_list if len(word) > 2]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "for index, row in df.iterrows():\n",
    "    if is_relevant(row['spacy_lemmas']) and row['entities'] != '':\n",
    "        TP += 1\n",
    "    elif is_relevant(row['spacy_lemmas']) and row['entities'] == '':\n",
    "        FP += 1\n",
    "    elif not is_relevant(row['spacy_lemmas']) and row['entities'] == '':\n",
    "        TN += 1\n",
    "    elif not is_relevant(row['spacy_lemmas']) and row['entities'] != '':\n",
    "        FN += 1\n",
    "print(TP, TN, FP, FN)\n",
    "array = [[TP, TN],\n",
    "        [FP, FN]]\n",
    "plot = make_confusion_matrix(np.array(array),\n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      filename='../plots/rules_matrix_prefilter.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use pretrained NER to prefilter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def is_relevant(row):\n",
    "    return bool({ent.label_ for ent in row['nlp'].ents}.intersection({'DATE' ,'ORG', 'PERSON', 'NORP', 'GPE', 'FAC', 'PRODUCT'}))\n",
    "\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "for index, row in df.iterrows():\n",
    "    if is_relevant(row) and row['entities'] != '':\n",
    "        TP += 1\n",
    "    elif is_relevant(row) and row['entities'] == '':\n",
    "        FP += 1\n",
    "    elif not is_relevant(row) and row['entities'] == '':\n",
    "        TN += 1\n",
    "    elif not is_relevant(row) and row['entities'] != '':\n",
    "        print(row['nlp'].ents, [ent.label_ for ent in row['nlp'].ents], row['entities'], row['nlp'].text)\n",
    "        FN += 1\n",
    "print(TP, TN, FP, FN)\n",
    "array = [[TP, TN],\n",
    "        [FP, FN]]\n",
    "plot = make_confusion_matrix(np.array(array),\n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      filename='../plots/ner_matrix_prefilter.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Draw confusion matricies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(array, index = [i for i in \"TF\"],\n",
    "                  columns = [i for i in \"PN\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "array = [[TN, FP],\n",
    "        [FN, TP]]\n",
    "from prefilter_results_raw import make_confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['Zero', 'One']\n",
    "plot = make_confusion_matrix(np.array(array),\n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      filename='../plots/test.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None,\n",
    "                          filename=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "\n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    filename:      Filename.\n",
    "    '''\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names) == cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten() / np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels, group_counts, group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0], cf.shape[1])\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        # Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        # if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf) == 2:\n",
    "            # Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1, 1] / sum(cf[:, 1])\n",
    "            recall = cf[1, 1] / sum(cf[1, :])\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy, precision, recall, f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize == None:\n",
    "        # Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks == False:\n",
    "        # Do not show categories if xyticks is False\n",
    "        categories = False\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf, annot=box_labels, fmt=\"\", cmap=cmap, cbar=cbar, xticklabels=categories, yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}